{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5400bf4b-b7c2-43ed-868a-8dabf90ab503",
   "metadata": {},
   "source": [
    "# Preprocessing an audio dataset\n",
    "\n",
    "* Resampling the audio data\n",
    "* Filtering the dataset\n",
    "* Converting audio data to modelâ€™s expected input\n",
    "\n",
    "You may also want to read Sanchi Gandhi's [A Complete Guide to Audio Datasets](https://huggingface.co/blog/audio-datasets#a-complete-guide-to-audio-datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46ad8d-1c5c-4515-98ef-b91991b47cba",
   "metadata": {},
   "source": [
    "### Resampling the audio data\n",
    "\n",
    "c.f. [https://huggingface.co/learn/audio-course/chapter1/preprocessing#resampling-the-audio-data](https://huggingface.co/learn/audio-course/chapter1/preprocessing#resampling-the-audio-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dbd93a-1250-4e43-89e2-c391629ac23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'transcription', 'intent_class'],\n",
       "    num_rows: 654\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up the minds dataset previously used...\n",
    "from datasets import load_dataset\n",
    "\n",
    "minds = load_dataset(\n",
    "    \"PolyAI/minds14\", \n",
    "    name=\"en-AU\", \n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6947b5ce-c83b-4357-8e66-656a4ce449ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate is 8000, with 62415 audio samples\n"
     ]
    }
   ],
   "source": [
    "#print(minds[0])\n",
    "\n",
    "print(\n",
    "    f\"Sampling rate is {minds[0]['audio']['sampling_rate']}, \"\n",
    "    f\"with {len(minds[0]['audio']['array'])} audio samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8e9f6-570e-40b9-bd1b-eddbd605c7a1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5935e6-cf6d-47f1-9620-f3b5e40ece88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\n",
    "    \"audio\", \n",
    "    Audio(sampling_rate=16_000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc86980-884c-4e95-a35b-8176bddddd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate is 16000, with 124830 audio samples\n"
     ]
    }
   ],
   "source": [
    "#print(minds[0])\n",
    "\n",
    "print(\n",
    "    f\"Sampling rate is {minds[0]['audio']['sampling_rate']}, \"\n",
    "    f\"with {len(minds[0]['audio']['array'])} audio samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825adccc-aed4-4296-a418-4b78caf004ba",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f8f69-5476-4dc0-9c4c-9d2f9dd229a2",
   "metadata": {},
   "source": [
    "### Filtering the dataset\n",
    "\n",
    "c.f. [https://huggingface.co/learn/audio-course/chapter1/preprocessing#filtering-the-dataset](https://huggingface.co/learn/audio-course/chapter1/preprocessing#filtering-the-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fec43db-0a4d-4644-9811-550417e6121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple filter function\n",
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a80cfc8-3ee7-46c8-841c-12b374c2a2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf931f82eaff40b89670d1b49955f2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'transcription', 'intent_class'],\n",
       "    num_rows: 624\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd315d14-4a70-46f2-97b0-06954d483125",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd96af-f5bd-4f50-b353-d94af2a7137d",
   "metadata": {},
   "source": [
    "### Pre-processing audio data\n",
    "\n",
    "c.f. [https://huggingface.co/learn/audio-course/chapter1/preprocessing#pre-processing-audio-data](https://huggingface.co/learn/audio-course/chapter1/preprocessing#pre-processing-audio-data)\n",
    "\n",
    "> The requirements for the input features may vary from one model to another â€” they depend on the modelâ€™s architecture, and the data it was pre-trained with. The good news is, for every supported audio model, ðŸ¤— Transformers offer a feature extractor class that can convert raw audio data into the input features the model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088162f-0160-435e-8ba3-b0a0d2114d9a",
   "metadata": {},
   "source": [
    "We are looking at the Whisper family of ASR models.\n",
    "\n",
    "* Whisper feature extractor pads/truncates a batch of audio examples such that all examples have an input length of 30s. Examples shorter than this are padded to 30s by appending zeros to the end of the sequence (zeros in an audio signal correspond to no signal or silence). Examples longer than 30s are truncated to 30s.\n",
    "* Since all elements in the batch are padded/truncated to a maximum length in the input space, there is no need for an attention mask. Whisper is unique in this regard, most other audio models require an attention mask that details where sequences have been padded, and thus where they should be ignored in the self-attention mechanism.\n",
    "* The second operation that the Whisper feature extractor performs is converting the padded audio arrays to log-mel spectrograms.\n",
    "\n",
    "c.f. [https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperFeatureExtractor](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperFeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12e9925-e953-4fc5-bcdc-4a5a2ffb384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/foo/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104ed8a38c5840108861516dae1196ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aebca5-ea97-4dc0-93ae-338cb57bc43d",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d329f2-06a8-4430-9f31-9f90bca1c12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bb15c-0892-4326-a74a-31046098389f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
